{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d965b3d",
   "metadata": {},
   "source": [
    "# HF Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f296f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "import time, gc, torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c0e18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea9b0582b0246608ab67e74e0cc5848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/721 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f595f530e2401a997bf3825327be80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32f5d3d0c4a42a0a862ff843be1fd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6317ac72b046eaabac5b67f5b4958f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"facebook/opt-13b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "kwargs = dict(\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a958c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae23aaf5f8e847bcb638f460c9e67e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/719 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570644ddae3d4e3fbcd39a429d5f17bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/52.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b01ccea4beb4643869ed59897be5b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dcb1de6bed9416496347461dc206dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5834cf74d734158a22c2e8a98ef42e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00003.bin:   0%|          | 0.00/9.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5717ef4180a4a05b1bc8dd74c7d5b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00003.bin:   0%|          | 0.00/5.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs)\n",
    "model = model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef9e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"In a galaxy far, far away\"] * 1\n",
    "input_tokens = tokenizer.batch_encode_plus(inputs, return_tensors=\"pt\", padding=True)\n",
    "for t in input_tokens:\n",
    "    if torch.is_tensor(input_tokens[t]):\n",
    "        input_tokens[t] = input_tokens[t].to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23695c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_kwargs = dict(max_new_tokens=100, use_cache=True, do_sample=False)\n",
    "output_tokens = model.generate(**input_tokens, **generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b008809",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.batch_decode(output_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6974eff",
   "metadata": {},
   "source": [
    "### RECREATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8efc7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0263a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_saver = {}\n",
    "shapes = [10, 128, 256, 512, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1ea53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_kwargs = model.generation_config.update(**input_tokens, **generate_kwargs)\n",
    "inputs_tensor, model_input_name, model_kwargs = model._prepare_model_inputs(\n",
    "    None, model.generation_config.bos_token_id, model_kwargs\n",
    ")\n",
    "model_kwargs[\"attention_mask\"] = model._prepare_attention_mask_for_generation(\n",
    "    inputs_tensor, model.generation_config.pad_token_id, model.generation_config.eos_token_id\n",
    ")\n",
    "model_kwargs[\"output_attentions\"] = model.generation_config.output_attentions\n",
    "model_kwargs[\"output_hidden_states\"] = model.generation_config.output_hidden_states\n",
    "model_kwargs[\"use_cache\"] = model.generation_config.use_cache\n",
    "\n",
    "input_ids = inputs_tensor\n",
    "\n",
    "model = model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(1024)):\n",
    "        if input_ids.shape[1] in shapes:\n",
    "            inputs_saver[input_ids.shape[1]] = {\n",
    "                \"model_kwargs\": model_kwargs.copy(),\n",
    "                \"input_ids\": input_ids,\n",
    "            }\n",
    "        \n",
    "        model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "        \n",
    "        outputs = model(\n",
    "            **model_inputs, \n",
    "            return_dict=True,\n",
    "            output_attentions=model.generation_config.output_attentions,\n",
    "            output_hidden_states=model.generation_config.output_hidden_states)\n",
    "\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "        model_kwargs = model._update_model_kwargs_for_generation(\n",
    "            outputs, model_kwargs, is_encoder_decoder=False\n",
    "        )        \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67645e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102ca438",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.batch_decode(output_tokens))\n",
    "print(tokenizer.batch_decode(input_ids[:,:108]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836d9166",
   "metadata": {},
   "source": [
    "## PROFILING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fe479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3136952a",
   "metadata": {},
   "source": [
    "### DECODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d4e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "for shape in shapes:\n",
    "    input_ids = inputs_saver[shape][\"input_ids\"]\n",
    "    model_kwargs = inputs_saver[shape][\"model_kwargs\"]\n",
    "    model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        print(f'KV Cache Shape: {model_inputs[\"past_key_values\"][0][0].shape}')\n",
    "        for it in tqdm(range(iterations)):\n",
    "            outputs = model(\n",
    "                **model_inputs, \n",
    "                return_dict=True,\n",
    "                output_attentions=model.generation_config.output_attentions,\n",
    "                output_hidden_states=model.generation_config.output_hidden_states)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.perf_counter()\n",
    "    \n",
    "    print(f\"Decode with input_ids.shape = {input_ids.shape}\")\n",
    "    print(f\"Time: {end-start: .2f}\")\n",
    "    print(f\"Iterations: {iterations}\")\n",
    "    print(f\"Throughput (tokens/sec): {iterations / (end-start) : .2f}\")\n",
    "    print(f\"Latency (sec/token): {(end-start) / iterations : .3f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b62cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
